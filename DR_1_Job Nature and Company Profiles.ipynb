{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Nature and Company Profiles (Topic Modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#What are the types of jobs that are in demand in Armenia? How are the job natures changing over time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#On the business question of Job Nature and Company Profiles. Unsupervised learning techniques, such as topic modelling and other techniques such as term frequency counting will be applied to the data, including time period segmented dataset. Qualitative assessment will be done on the results to help us understand the job postings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import logging\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori = pd.read_csv('C:\\\\Users\\\\deeparam\\\\Desktop\\\\DMCA\\\\Sols_DSP_Capstone_Projects_PS_Chegg_2019\\\\6. TEXT MINING - ONLINE JOB POSTING\\data job posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori.head()\n",
    "print(df_ori.shape)\n",
    "df = df_ori.drop_duplicates(['jobpost', 'Title'])\n",
    "print(df.shape)\n",
    "print(\"Removed {0} duplicates (based on jobpost + Title)\".format(df_ori.shape[0] - df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokens):\n",
    "    tokens_nop = [t for t in tokens if t not in string.punctuation]\n",
    "    tokens_nop = [t.lower() for t in tokens_nop]\n",
    "    # wnl = nltk.WordNetLemmatizer()\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(\n",
    "        ['armenian', 'armenia', 'job', 'title', 'position', 'location', 'responsibilities', 'application', 'procedures',\n",
    "         'deadline', 'required', 'qualifications', 'renumeration', 'salary', 'date', 'company', 'yerevan',\n",
    "         'eligibility', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september',\n",
    "         'october', 'november', 'december'])\n",
    "    tokens_nostop = [t for t in tokens_nop if t not in stop]\n",
    "    # tokens_lem = [wnl.lemmatize(t) for t in tokens_nostop]\n",
    "    tokens_clean = [t for t in tokens_nostop if len(t) >= 3]  # simple way to remove the offending \" punctuations\n",
    "    return tokens_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotWC(tokens):\n",
    "    text_clean = \" \".join(tokens)\n",
    "    print(text_clean)\n",
    "    wc = WordCloud(background_color=\"white\").generate(text_clean)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    fd = nltk.FreqDist(tokens)  # case\n",
    "    fd.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#import en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "  \n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub('\\S*@\\S*\\s?', '', x))  # remove emails\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub('\\s+', ' ', x))  # remove newlines\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub(\"\\'\", \"\", x))  # remove single quotes\n",
    "re1 = '(www)'  # Word 1\n",
    "re2 = '(\\\\.)'  # Any Single Character 1\n",
    "re3 = '((?:[a-z][a-z0-9_]*))'  # Variable Name 1\n",
    "re4 = '(\\\\.)'  # Any Single Character 2\n",
    "re5 = '((?:[a-z][a-z0-9_]*))'  # Variable Name 2\n",
    "rg = re.compile(re1 + re2 + re3 + re4 + re5, re.IGNORECASE | re.DOTALL)\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub(rg, \"\", x))\n",
    "re1 = '((?:[a-z][a-z0-9_]*))'  # Variable Name 1\n",
    "re2 = '(\\\\.)'  # Any Single Character 1\n",
    "re3 = '((?:[a-z][a-z0-9_]*))'  # Word 1\n",
    "rg = re.compile(re1 + re2 + re3, re.IGNORECASE | re.DOTALL)\n",
    "df['jobpost'] = df['jobpost'].apply(lambda x: re.sub(rg, \"\", x))\n",
    "df.jobpost = df.jobpost.apply(lambda x: re.sub('(\\\\d+)', \"\", x))  # remove numbers\n",
    "\n",
    "df['jobpost_token'] = df.jobpost.map(word_tokenize)\n",
    "df['jobpost_len'] = df.jobpost_token.apply(len)\n",
    "df['jobpost_token_uniq'] = df.jobpost_token.apply(set)\n",
    "df['jobpost_processed'] = df.jobpost_token.apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "\n",
    "bigram = gensim.models.Phrases(df['jobpost_processed'], min_count=5, threshold=100) # higher threshold fewer phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = gensim.models.Phrases(bigram['jobpost_processed'], threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.jobpost_processed = make_bigrams(df.jobpost_processed)\n",
    "df.jobpost_processed = lemmatization(df.jobpost_processed, allowed_postags=['NOUN', 'VERB'])  # 'ADJ',, 'ADV'])\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "dictionary = corpora.Dictionary(df['jobpost_processed'])\n",
    "#print(dictionary)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.7)\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_num = 7\n",
    "\n",
    "# Use the dictionary to prepare a DTM (using TF)\n",
    "dtm_train = [dictionary.doc2bow(d) for d in df['jobpost_processed']]\n",
    "lda = gensim.models.ldamodel.LdaModel(dtm_train, num_topics=topic_num, alpha='auto', chunksize=30, id2word=dictionary,\n",
    "                                      passes=20, random_state=432)\n",
    "# lda.show_topics()\n",
    "# lda.show_topics(num_words=20)\n",
    "\n",
    "dtopics_train = lda.get_document_topics(dtm_train)\n",
    "# print topic distribution for 1st 5 rows\n",
    "for i in range(0, 5):\n",
    "    print(dtopics_train[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lda.print_topics(): \n",
    "    for j in i: print (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7 topics\n",
    "\n",
    "0, Sales and Marketing \n",
    "\n",
    "1, Project Management and Development \n",
    "\n",
    "2, Banking and Finance\n",
    "\n",
    "3, Software Development \n",
    "\n",
    "4, Construction and Safety Engineering \n",
    "\n",
    "5, Education and Training \n",
    "\n",
    "6, Business Development and Management \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and plot the distribution of the topics\n",
    "from operator import itemgetter\n",
    "import matplotlib.style as style\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# style.use('seaborn-poster')\n",
    "# fig, ax = plt.subplots(figsize=(16, 9))\n",
    "top_train = [max(t, key=itemgetter(1))[0] for t in dtopics_train]\n",
    "# plt.hist(top_train, bins=topic_num)\n",
    "# plt.title('Topic Frequencies')\n",
    "# plt.show()\n",
    "sns.distplot(top_train, kde=False).set_title('Topic Frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.Series(top_train)\n",
    "df['topic'] = topics.values\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_04to07 = df[df.Year <= 2007]\n",
    "sns.distplot(df_04to07['topic'], kde=False).set_title('Topic Frequencies 2004 - 2007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_08to11 = df[(df['Year'] > 2007) & (df['Year'] <= 2011)]\n",
    "sns.distplot(df_08to11['topic'], kde=False).set_title('Topic Frequencies 2008 - 2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12to15 = df[(df['Year'] > 2011) & (df['Year'] <= 2015)]\n",
    "sns.distplot(df_12to15['topic'], kde=False).set_title('Topic Frequencies 2012 - 2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#As can be observed, proportion-wise, the topic frequencies remain roughly similar. What is obvious is that the two topics Banking and Finance as well as Software Development have grown in proportion across the overall job market over the years. Also, the highest number of jobs is for a Sales and Marketing position. Construction and Safety Engineering kinds of job consistently had low numbers of job postings over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#There were slightly less Software Development jobs compared to Banking and Finance jobs initially from 2004-2007. This trend continued from 2008 to 2011. However, from 2012-2015, Software Development jobs were higher in demand than Banking and Finance jobs.\n",
    "\n",
    "A possible explanation could be that banks and finance services have become more digitalised, and many jobs for the Banking and Finance industry require Software Development skills as well.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
